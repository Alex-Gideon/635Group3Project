{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/stu4/s12/asg9582/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/stu4/s12/asg9582/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/stu4/s12/asg9582/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home/stu4/s12/asg9582/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet \n",
    "from os import walk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "import csv\n",
    "from string import punctuation\n",
    "import re\n",
    "from bs4 import BeautifulSoup as beauty\n",
    "from gensim.models import KeyedVectors\n",
    "from itertools import chain\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/tweets-1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3059981/697714469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/tweets-1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweets2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/tweets-2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0munique_sentiments_tweets1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0munique_sentiments_tweets2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/635Proj/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/635Proj/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/635Proj/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/635Proj/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/635Proj/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/635Proj/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m         )\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/635Proj/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m             )\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/tweets-1.csv'"
     ]
    }
   ],
   "source": [
    "tweets1 = pd.read_csv('datasets/tweets-1.csv')\n",
    "tweets2 = pd.read_csv('datasets/tweets-2.csv')\n",
    "\n",
    "unique_sentiments_tweets1 = tweets1['sentiment'].value_counts()\n",
    "unique_sentiments_tweets2 = tweets2['category'].value_counts()\n",
    "print(\"Unique sentiment counts in tweets1:\")\n",
    "print(unique_sentiments_tweets1)\n",
    "print(\"\\nUnique sentiment counts in tweets2:\")\n",
    "print(unique_sentiments_tweets2)\n",
    "display(tweets1)\n",
    "display(tweets2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets to AT-LSTM Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive    80831\n",
      "negative    43290\n",
      "Name: sentiment, dtype: int64\n",
      "text         0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107762</th>\n",
       "      <td>engine growth modi unveils indias first 12000 ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107763</th>\n",
       "      <td>modi promised 2014 lok sabha elections that be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107764</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107765</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107766</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124121 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text sentiment\n",
       "0           Sooo SAD I will miss you here in San Diego!!!  negative\n",
       "1                               my boss is bullying me...  negative\n",
       "2                          what interview! leave me alone  negative\n",
       "3        Sons of ****, why couldn`t they put them on t...  negative\n",
       "4       2am feedings for the baby are fun when he is a...  positive\n",
       "...                                                   ...       ...\n",
       "107762  engine growth modi unveils indias first 12000 ...  positive\n",
       "107763  modi promised 2014 lok sabha elections that be...  positive\n",
       "107764  why these 456 crores paid neerav modi not reco...  negative\n",
       "107765  dear rss terrorist payal gawar what about modi...  negative\n",
       "107766  have you ever listen about like gurukul where ...  positive\n",
       "\n",
       "[124121 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets1_atlstm = tweets1[tweets1['sentiment'] != 'neutral'][['text', 'sentiment']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "tweets2_atlstm = tweets2[tweets2['category'] != 0][['clean_text', 'category']].copy()\n",
    "tweets2_atlstm['sentiment'] = tweets2_atlstm['category'].map({-1.0: 'negative', 0.0: 'neutral', 1.0: 'positive'})\n",
    "tweets2_atlstm = tweets2_atlstm[['clean_text', 'sentiment']].reset_index(drop=True)\n",
    "tweets2_atlstm.rename(columns={'clean_text': 'text'}, inplace=True)\n",
    "\n",
    "unique_sentiments_tweets1 = tweets1_atlstm['sentiment'].value_counts()\n",
    "unique_sentiments_tweets2 = tweets2_atlstm['sentiment'].value_counts()\n",
    "tweets1_atlstm.dropna(inplace=True)\n",
    "tweets2_atlstm.dropna(inplace=True)\n",
    "combined_df = pd.concat([tweets1_atlstm, tweets2_atlstm])\n",
    "print(combined_df['sentiment'].value_counts())\n",
    "missing_values_count = combined_df.isnull().sum()\n",
    "print(missing_values_count)\n",
    "combined_df.rename(columns={'text': 'review'}).to_csv('datasets/atlstm_tweets.csv', index=False)\\\n",
    "\n",
    "display(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets to Bi_LSTM and ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_html_tags(row):\n",
    "    return beauty(row, 'html.parser').text\n",
    "\n",
    "\n",
    "def tokenize(input_text):\n",
    "    tokens = re.sub('[^a-zA-Z]', ' ', input_text).lower().split()\n",
    "\n",
    "    # tokens = word_tokenizer.tokenize(input_text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_stop_words(input_text_vector):\n",
    "    filtered = []\n",
    "    for word in input_text_vector:\n",
    "        if word.isalpha() and word not in stop_words:\n",
    "            filtered.append(word)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def standardize(input_label):\n",
    "    return 1 if input_label == 'positive' else 0\n",
    "\n",
    "\n",
    "def all_at_once(input_text):\n",
    "    cleaned = remove_html_tags(input_text)\n",
    "    cleaned = tokenize(cleaned)\n",
    "    cleaned = remove_stop_words(cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def hypernym_list(tokens):\n",
    "    hypernym_tokens = []\n",
    "    for word in tokens:\n",
    "        My_sysn = wordnet.synsets(word)\n",
    "        if len(My_sysn) == 0:\n",
    "            hypernym_tokens.append(word)\n",
    "        else:\n",
    "            hypernym_tokens.append(My_sysn[0].lemma_names()[0])\n",
    "    return hypernym_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu4/s12/asg9582/miniconda3/envs/635Proj/lib/python3.7/site-packages/ipykernel_launcher.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized\n",
      "hypernymed 1...\n",
      "hypernymed 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>hypernym</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>[sooo, sad, miss, san, diego]</td>\n",
       "      <td>[sooo, sad, girl, san, diego]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[boss, bullying]</td>\n",
       "      <td>[foreman, bullying]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>[interview, leave, alone]</td>\n",
       "      <td>[interview, leave, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[sons, put, releases, already, bought]</td>\n",
       "      <td>[son, put_option, release, already, buy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[feedings, baby, fun, smiles, coos]</td>\n",
       "      <td>[eating, baby, fun, smile, coo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16358</th>\n",
       "      <td>enjoy ur night</td>\n",
       "      <td>positive</td>\n",
       "      <td>[enjoy, ur, night]</td>\n",
       "      <td>[enjoy, Ur, night]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16359</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[wish, could, come, see, u, denver, husband, l...</td>\n",
       "      <td>[wish, could, semen, see, uracil, Denver, husb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16360</th>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[wondered, rake, client, made, clear, net, for...</td>\n",
       "      <td>[wonder, rake, client, make, clear, internet, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16361</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[yay, good, enjoy, break, probably, need, hect...</td>\n",
       "      <td>[Yay, good, enjoy, interruption, probably, nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16362</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "      <td>[worth]</td>\n",
       "      <td>[worth]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16363 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "0          Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "1                              my boss is bullying me...  negative   \n",
       "2                         what interview! leave me alone  negative   \n",
       "3       Sons of ****, why couldn`t they put them on t...  negative   \n",
       "4      2am feedings for the baby are fun when he is a...  positive   \n",
       "...                                                  ...       ...   \n",
       "16358                                     enjoy ur night  positive   \n",
       "16359   wish we could come see u on Denver  husband l...  negative   \n",
       "16360   I`ve wondered about rake to.  The client has ...  negative   \n",
       "16361   Yay good for both of you. Enjoy the break - y...  positive   \n",
       "16362                         But it was worth it  ****.  positive   \n",
       "\n",
       "                                               tokenized  \\\n",
       "0                          [sooo, sad, miss, san, diego]   \n",
       "1                                       [boss, bullying]   \n",
       "2                              [interview, leave, alone]   \n",
       "3                 [sons, put, releases, already, bought]   \n",
       "4                    [feedings, baby, fun, smiles, coos]   \n",
       "...                                                  ...   \n",
       "16358                                 [enjoy, ur, night]   \n",
       "16359  [wish, could, come, see, u, denver, husband, l...   \n",
       "16360  [wondered, rake, client, made, clear, net, for...   \n",
       "16361  [yay, good, enjoy, break, probably, need, hect...   \n",
       "16362                                            [worth]   \n",
       "\n",
       "                                                hypernym  \n",
       "0                          [sooo, sad, girl, san, diego]  \n",
       "1                                    [foreman, bullying]  \n",
       "2                              [interview, leave, alone]  \n",
       "3               [son, put_option, release, already, buy]  \n",
       "4                        [eating, baby, fun, smile, coo]  \n",
       "...                                                  ...  \n",
       "16358                                 [enjoy, Ur, night]  \n",
       "16359  [wish, could, semen, see, uracil, Denver, husb...  \n",
       "16360  [wonder, rake, client, make, clear, internet, ...  \n",
       "16361  [Yay, good, enjoy, interruption, probably, nee...  \n",
       "16362                                            [worth]  \n",
       "\n",
       "[16363 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets1_atlstm['tokenized'] = tweets1_atlstm['text'].apply(lambda x: all_at_once(x))\n",
    "tweets2_atlstm['tokenized'] = tweets2_atlstm['text'].apply(lambda x: all_at_once(x))\n",
    "print(\"tokenized\")\n",
    "tweets1_atlstm['hypernym'] = tweets1_atlstm['tokenized'].apply(lambda x: hypernym_list(x))\n",
    "print(\"hypernymed 1...\")\n",
    "tweets2_atlstm['hypernym'] = tweets2_atlstm['tokenized'].apply(lambda x: hypernym_list(x))\n",
    "print(\"hypernymed 2\")\n",
    "display(tweets1_atlstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, tokenized_csv_file, hypernyms_csv_file):\n",
    "    # Convert sentiment from letter to number\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'negative' else 10)\n",
    "    \n",
    "    # Write sentiment and tokenized columns to CSV\n",
    "    with open(tokenized_csv_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "        for index, row in df.iterrows():\n",
    "            sentiment = str(row['sentiment'])\n",
    "            tokenized = ','.join(row['tokenized'])\n",
    "            writer.writerow([sentiment, tokenized])\n",
    "    \n",
    "    # Write hypernyms column to CSV\n",
    "    with open(hypernyms_csv_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "        for index, row in df.iterrows():\n",
    "            hypernyms = ','.join(row['hypernym'])\n",
    "            if not hypernyms:\n",
    "                print(f\"Empty hypernyms list encountered at index {index}\")\n",
    "                print(row)\n",
    "            writer.writerow([hypernyms])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text         0\n",
      "sentiment    0\n",
      "tokenized    0\n",
      "hypernym     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nullcount1 = tweets1_atlstm.isnull().sum()\n",
    "print(nullcount1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1_atlstm = tweets1_atlstm[(tweets1_atlstm['tokenized'].apply(len) > 0) & (tweets1_atlstm['hypernym'].apply(len) > 0)]\n",
    "tweets2_atlstm = tweets2_atlstm[(tweets2_atlstm['tokenized'].apply(len) > 0) & (tweets2_atlstm['hypernym'].apply(len) > 0)]\n",
    "\n",
    "process_dataframe(tweets1_atlstm,  \"datasets/small_tweets_bilstm.csv\", \"datasets/small_tweets_ANN.csv\")\n",
    "process_dataframe(tweets2_atlstm, \"datasets/big_tweets_bilstm.csv\", \"datasets/big_tweets_ANN.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "635Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
